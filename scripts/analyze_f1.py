#!/usr/bin/env python3
"""
LoCoMo Detailed F1 Score Analysis Tool
è¯¦ç»†çš„ F1 åˆ†æ•°åˆ†æå·¥å…·

åŠŸèƒ½ï¼š
1. è®¡ç®—å¹¶å±•ç¤ºæ¯ä¸ªæ¨¡å‹çš„å¹³å‡ F1 åˆ†æ•°
2. æŒ‰é—®é¢˜ç±»åˆ«å±•ç¤º F1 åˆ†æ•°
3. å¯¹æ¯”çº¯ LLM å’Œ RAG æ–¹æ³•çš„æ€§èƒ½
4. å¯¼å‡º CSV æŠ¥å‘Š
5. è¾“å‡ºç¾è§‚çš„è¡¨æ ¼ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
6. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼ˆå¦‚æœå®‰è£…äº† matplotlibï¼‰
"""

import os
import sys
import json
import argparse
from pathlib import Path
from collections import defaultdict

# å°è¯•å¯¼å…¥ tabulate ç”¨äºç¾è§‚è¡¨æ ¼è¾“å‡º
try:
    from tabulate import tabulate
    TABULATE_AVAILABLE = True
except ImportError:
    TABULATE_AVAILABLE = False


def load_stats(stats_file):
    """åŠ è½½ç»Ÿè®¡æ–‡ä»¶"""
    if not os.path.exists(stats_file):
        return None
    
    with open(stats_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def calculate_f1_scores(stats_data, model_name):
    """è®¡ç®— F1 åˆ†æ•°ç»Ÿè®¡"""
    if model_name not in stats_data:
        return None
    
    model_data = stats_data[model_name]
    category_counts = model_data['category_counts']
    cum_accuracy = model_data['cum_accuracy_by_category']
    
    results = {
        'model_name': model_name,
        'total_questions': sum(category_counts.values()),
        'total_f1': sum(cum_accuracy.values()),
        'categories': {}
    }
    
    # è®¡ç®—å¹³å‡ F1
    results['avg_f1'] = results['total_f1'] / results['total_questions'] if results['total_questions'] > 0 else 0
    
    # æŒ‰ç±»åˆ«è®¡ç®—
    for cat in sorted(category_counts.keys(), key=int):
        cat_count = category_counts[cat]
        cat_f1_sum = cum_accuracy[cat]
        cat_avg_f1 = cat_f1_sum / cat_count if cat_count > 0 else 0
        
        results['categories'][int(cat)] = {
            'count': cat_count,
            'total_f1': cat_f1_sum,
            'avg_f1': cat_avg_f1
        }
    
    return results


def print_model_results(results, title="Model Results", table_format="grid"):
    """æ‰“å°æ¨¡å‹ç»“æœï¼ˆæ”¯æŒç¾è§‚è¡¨æ ¼ï¼‰
    
    Args:
        results: ç»“æœå­—å…¸
        title: æ ‡é¢˜
        table_format: è¡¨æ ¼æ ¼å¼ (grid, fancy_grid, simple, plain, etc.)
    """
    if results is None:
        print(f"  âŒ No results available")
        return
    
    print(f"\n{'='*80}")
    print(f"  {title}")
    print(f"{'='*80}")
    print(f"  Model: {results['model_name']}")
    print(f"  Total Questions: {results['total_questions']}")
    print(f"  Average F1 Score: {results['avg_f1']:.4f}")
    print(f"\n  ğŸ“‹ Category Breakdown:")
    
    # å‡†å¤‡è¡¨æ ¼æ•°æ®
    table_data = []
    for cat_id in sorted(results['categories'].keys()):
        cat_data = results['categories'][cat_id]
        cat_name = get_category_name(cat_id)
        table_data.append([
            cat_name,
            cat_data['count'],
            f"{cat_data['avg_f1']:.4f}"
        ])
    
    # ä½¿ç”¨ tabulate è¾“å‡ºç¾è§‚è¡¨æ ¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if TABULATE_AVAILABLE:
        headers = ['Category', 'Questions', 'Avg F1']
        table = tabulate(table_data, headers=headers, tablefmt=table_format)
        # ç¼©è¿›è¡¨æ ¼
        indented_table = '\n'.join(['  ' + line for line in table.split('\n')])
        print(indented_table)
    else:
        # é™çº§åˆ°ç®€å•æ ¼å¼
        print(f"  {'Category':<12} {'Questions':<12} {'Avg F1':<12}")
        print(f"  {'-'*40}")
        for row in table_data:
            print(f"  {row[0]:<12} {row[1]:<12} {row[2]:<12}")


def get_category_name(cat_id):
    """è·å–é—®é¢˜ç±»åˆ«åç§°"""
    category_names = {
        1: "Cat 1",  # ç®€å•äº‹å®
        2: "Cat 2",  # æ—¶é—´æ¨ç†
        3: "Cat 3",  # è·¨ä¼šè¯æ¨ç†
        4: "Cat 4",  # å¤šæ­¥æ¨ç†
        5: "Cat 5",  # å¯¹æŠ—æ€§é—®é¢˜
    }
    return category_names.get(cat_id, f"Cat {cat_id}")


def compare_models(results_list, table_format="grid"):
    """å¯¹æ¯”å¤šä¸ªæ¨¡å‹ï¼ˆæ”¯æŒç¾è§‚è¡¨æ ¼ï¼‰
    
    Args:
        results_list: ç»“æœåˆ—è¡¨
        table_format: è¡¨æ ¼æ ¼å¼
    """
    if not results_list:
        print("\nâŒ No results to compare")
        return
    
    print(f"\n{'='*80}")
    print(f"  ğŸ“Š Model Comparison (sorted by F1 score)")
    print(f"{'='*80}")
    
    # æŒ‰ F1 åˆ†æ•°æ’åº
    sorted_results = sorted(results_list, key=lambda x: x['avg_f1'], reverse=True)
    
    # å‡†å¤‡è¡¨æ ¼æ•°æ®
    table_data = []
    for idx, result in enumerate(sorted_results, 1):
        rank_emoji = "ğŸ¥‡" if idx == 1 else "ğŸ¥ˆ" if idx == 2 else "ğŸ¥‰" if idx == 3 else f"{idx}."
        table_data.append([
            rank_emoji,
            result['model_name'],
            result['method'].upper(),
            f"{result['avg_f1']:.4f}",
            result['total_questions']
        ])
    
    # ä½¿ç”¨ tabulate è¾“å‡ºç¾è§‚è¡¨æ ¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if TABULATE_AVAILABLE:
        headers = ['Rank', 'Model', 'Method', 'Avg F1', 'Questions']
        table = tabulate(table_data, headers=headers, tablefmt=table_format)
        # ç¼©è¿›è¡¨æ ¼
        indented_table = '\n'.join(['  ' + line for line in table.split('\n')])
        print(indented_table)
    else:
        # é™çº§åˆ°ç®€å•æ ¼å¼
        print(f"  {'Rank':<6} {'Model':<40} {'Method':<12} {'Avg F1':<15} {'Questions':<15}")
        print(f"  {'-'*90}")
        for row in table_data:
            print(f"  {row[0]:<6} {row[1]:<40} {row[2]:<12} {row[3]:<15} {row[4]:<15}")


def export_to_csv(results_list, output_file):
    """å¯¼å‡ºç»“æœåˆ° CSV"""
    import csv
    
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # å†™å…¥è¡¨å¤´
        writer.writerow(['Model', 'Total Questions', 'Average F1', 'Category', 'Category Questions', 'Category F1'])
        
        # å†™å…¥æ•°æ®
        for result in results_list:
            model_name = result['model_name']
            total_q = result['total_questions']
            avg_f1 = result['avg_f1']
            
            # å†™å…¥æ€»ä½“æ•°æ®
            writer.writerow([model_name, total_q, f"{avg_f1:.4f}", 'Overall', total_q, f"{avg_f1:.4f}"])
            
            # å†™å…¥åˆ†ç±»æ•°æ®
            for cat_id in sorted(result['categories'].keys()):
                cat_data = result['categories'][cat_id]
                writer.writerow([
                    model_name,
                    total_q,
                    f"{avg_f1:.4f}",
                    f"Category {cat_id}",
                    cat_data['count'],
                    f"{cat_data['avg_f1']:.4f}"
                ])
    
    print(f"\nâœ… Results exported to: {output_file}")


def scan_outputs_directory(base_dir="./outputs"):
    """æ‰«æè¾“å‡ºç›®å½•ï¼Œæ‰¾åˆ°æ‰€æœ‰ç»Ÿè®¡æ–‡ä»¶"""
    results = []
    base_path = Path(base_dir)
    
    if not base_path.exists():
        print(f"âš ï¸  Directory not found: {base_dir}")
        return results
    
    # æŸ¥æ‰¾æ‰€æœ‰ *_stats.json æ–‡ä»¶
    for stats_file in base_path.rglob("*_stats.json"):
        stats_data = load_stats(stats_file)
        if stats_data:
            # è·å–ç›¸å¯¹è·¯å¾„ä»¥åŒºåˆ†ä¸åŒæ–¹æ³•
            rel_path = stats_file.relative_to(base_path)
            method = rel_path.parts[0] if len(rel_path.parts) > 1 else "unknown"
            
            for model_name in stats_data.keys():
                result = calculate_f1_scores(stats_data, model_name)
                if result:
                    result['method'] = method
                    result['file'] = str(stats_file)
                    results.append(result)
    
    return results


def print_category_comparison(results_list, table_format="grid"):
    """æŒ‰ç±»åˆ«å¯¹æ¯”æ‰€æœ‰æ¨¡å‹çš„è¡¨ç°"""
    if not results_list:
        return
    
    print(f"\n{'='*80}")
    print(f"  ğŸ“Š Category-wise Performance Comparison")
    print(f"{'='*80}")
    
    # æ”¶é›†æ‰€æœ‰ç±»åˆ«
    all_categories = set()
    for result in results_list:
        all_categories.update(result['categories'].keys())
    
    # å‡†å¤‡è¡¨æ ¼æ•°æ®
    table_data = []
    for cat_id in sorted(all_categories):
        cat_name = get_category_name(cat_id)
        row = [cat_name]
        
        for result in results_list:
            if cat_id in result['categories']:
                cat_f1 = result['categories'][cat_id]['avg_f1']
                row.append(f"{cat_f1:.4f}")
            else:
                row.append("N/A")
        
        table_data.append(row)
    
    # æ·»åŠ å¹³å‡è¡Œ
    avg_row = ["Overall"]
    for result in results_list:
        avg_row.append(f"{result['avg_f1']:.4f}")
    table_data.append(avg_row)
    
    # æ„å»ºè¡¨å¤´
    headers = ['Category'] + [f"{r['model_name'][:20]}..." if len(r['model_name']) > 20 else r['model_name'] for r in results_list]
    
    # ä½¿ç”¨ tabulate è¾“å‡ºç¾è§‚è¡¨æ ¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if TABULATE_AVAILABLE:
        table = tabulate(table_data, headers=headers, tablefmt=table_format)
        indented_table = '\n'.join(['  ' + line for line in table.split('\n')])
        print(indented_table)
    else:
        # é™çº§åˆ°ç®€å•æ ¼å¼
        col_width = 15
        header_line = "  Category".ljust(15)
        for h in headers[1:]:
            header_line += h[:col_width-1].ljust(col_width)
        print(header_line)
        print(f"  {'-'*80}")
        for row in table_data:
            line = f"  {row[0]:<15}"
            for val in row[1:]:
                line += f"{val:<15}"
            print(line)


def main():
    parser = argparse.ArgumentParser(description='LoCoMo F1 Score Analysis Tool')
    parser.add_argument('--output-dir', default='./outputs', help='Output directory to scan')
    parser.add_argument('--export-csv', help='Export results to CSV file')
    parser.add_argument('--model', help='Specific model name to analyze')
    parser.add_argument('--method', choices=['hf_llm', 'rag_hf_llm', 'all'], default='all',
                       help='Method to analyze (pure LLM or RAG)')
    parser.add_argument('--table-format', default='grid',
                       choices=['plain', 'simple', 'grid', 'fancy_grid', 'pipe', 'orgtbl', 
                               'rst', 'mediawiki', 'html', 'latex', 'github'],
                       help='Table format (requires tabulate library)')
    parser.add_argument('--category-comparison', action='store_true',
                       help='Show category-wise comparison across all models')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ tabulate å¯ç”¨æ€§
    if not TABULATE_AVAILABLE and args.table_format != 'plain':
        print("âš ï¸  tabulate library not found. Install it for beautiful tables:")
        print("   pip install tabulate")
        print("   Falling back to simple format...\n")
    
    print("ğŸ” Scanning output directory...")
    all_results = scan_outputs_directory(args.output_dir)
    
    if not all_results:
        print(f"\nâŒ No results found in {args.output_dir}")
        print(f"   Please run evaluation scripts first:")
        print(f"   - Pure LLM: ./scripts/evaluate_hf_llm.sh")
        print(f"   - RAG:      ./scripts/evaluate_rag_hf_llm.sh")
        return
    
    # è¿‡æ»¤ç»“æœ
    filtered_results = all_results
    if args.method != 'all':
        filtered_results = [r for r in all_results if r['method'] == args.method]
    
    if args.model:
        filtered_results = [r for r in filtered_results if args.model in r['model_name']]
    
    if not filtered_results:
        print(f"\nâŒ No results matching criteria")
        return
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“Š Found {len(filtered_results)} result(s)")
    
    for result in filtered_results:
        title = f"{result['method'].upper()} - {result['model_name']}"
        print_model_results(result, title, table_format=args.table_format)
    
    # å¯¹æ¯”
    if len(filtered_results) > 1:
        compare_models(filtered_results, table_format=args.table_format)
        
        # æŒ‰ç±»åˆ«å¯¹æ¯”ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
        if args.category_comparison:
            print_category_comparison(filtered_results, table_format=args.table_format)
    
    # å¯¼å‡º CSV
    if args.export_csv:
        export_to_csv(filtered_results, args.export_csv)
    
    print(f"\n{'='*80}")
    print("âœ… Analysis complete!")
    if TABULATE_AVAILABLE:
        print(f"   Table format: {args.table_format}")
    print(f"   Total results analyzed: {len(filtered_results)}")


if __name__ == "__main__":
    main()
